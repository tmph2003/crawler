import asyncio
import time

from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext
from common.db_helper import DatabaseHelper
from common.s3_helper import S3Helper
from common.network_helper import NetworkHelper
from config.config import config
from crawlee._request import Request, RequestOptions
from crawlee.proxy_configuration import ProxyConfiguration

s3 = S3Helper()
network_helper = NetworkHelper()
db_helper = DatabaseHelper(
    host=config.MARIADB_HOST,
    port=config.MARIADB_PORT,
    user=config.MARIADB_USER,
    password=config.MARIADB_PASS,
    database="mydatabase"
)   

def update_flag(table: str, link: str):
    """
    Set flag=True for the given link in the specified table.
    """
    sql = f"""
        UPDATE {table}
        SET flag = True
        WHERE link = '{link}'
    """
    db_helper.execute(sql)

async def failed_request_handler(context: BeautifulSoupCrawlingContext, error):
    status_code = getattr(error, "status_code", None)
    context.log.warning(f"âš ï¸ Request lá»—i ({status_code}), Ä‘á»•i proxy...")

    try:
        # Delay trÃ¡nh bá»‹ cháº·n tiáº¿p
        time.sleep(10)
        request = context.request

        request_options = RequestOptions(url=request.url, label=request.label, user_data=request.user_data, unique_key=request.unique_key)
        request = Request.from_url(**request_options)
        await context.add_requests(requests=[request], forefront=True)

    except Exception as e:
        context.log.error(f"âŒ KhÃ´ng Ä‘á»•i proxy Ä‘Æ°á»£c: {e}")

async def process_link(context: BeautifulSoupCrawlingContext):
    url = context.request.url
    soup = context.soup

    # Láº¥y list href
    hrefs = [
        str("https://masothue.com" + a.get("href"))
        for a in soup.select("#main > section > div > div.tax-listing > div > h3 > a")
    ]

    # Láº¥y list text (masothue)
    titles = [
        a.get_text(strip=True)
        for a in soup.select("#main > section > div > div.tax-listing > div > div > a")
    ]

    # GhÃ©p thÃ nh list tuple (masothue, link)
    data_company = [
        (t, h) for t, h in zip(titles, hrefs)
    ]

    sql = """
        INSERT INTO company_link (masothue, link)
        VALUES (%s, %s)
        ON DUPLICATE KEY UPDATE link = VALUES(link)
    """

    # Insert nhiá»u record cÃ¹ng lÃºc
    if data_company:
        db_helper.executemany(sql, data_company)
        print(f"âœ… INSERT {len(hrefs)} links tá»« {url}")
    if len(hrefs) == 25:
        if "?page=" not in url:
            url = url + "?page=1"
        base_url, current_page = url.split("?page=")
        current_page = int(current_page)
        if current_page >= 10:
            print(f"â›” Dá»«ng crawl táº¡i trang {current_page} cá»§a {base_url}")
            update_flag("ward", url)
            return
        next_page = current_page + 1
        next_url = f"{base_url}?page={next_page}"
        await context.add_requests([next_url], forefront=True)
    else:
        update_flag("ward", url)
        return



async def main():
    # host, port, user, password = network_helper.get_proxy()
    proxy_configuration = ProxyConfiguration(
        proxy_urls=[
            f"http://user12006:0sj4@103.179.188.215:12006/",
            f"http://user11809:0sj4@103.179.188.215:11809/"
        ],
    )
    crawler = BeautifulSoupCrawler(
        max_requests_per_crawl=10000,
        proxy_configuration=proxy_configuration
    )
    
    # host, port, user, password = network_helper.get_proxy()
    crawler.failed_request_handler(handler=failed_request_handler)

    crawler.router.default_handler(process_link)

    # láº¥y batch link tá»« DB
    rows = db_helper.execute("""
        SELECT link FROM ward
        WHERE flag = False
        ORDER BY link ASC
    """)
    urls = [row[0] for row in rows]
    print("=====================================")
    print(len(urls))
    print("=====================================")
    if rows:        
        batch_size = 10
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i+batch_size]
            print(f"ðŸš€ Cháº¡y batch {i//batch_size + 1}, gá»“m {len(batch)} links")
            # cháº¡y crawler vá»›i batch nÃ y
            await crawler.run(batch)

if __name__ == "__main__":
    asyncio.run(main())